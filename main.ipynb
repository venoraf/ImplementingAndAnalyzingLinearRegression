import torch
import numpy as np
from numpy import absolute
import math
import matplotlib.pyplot as plt
import random
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
from sklearn.linear_model import Ridge

#Generate the Data
def create_data(N):

  #creates the empty tensor
  X = torch.zeros(N, 101)
  y = torch.zeros(N, 1)

  for j in range(N):
    X[j, 0] = 1

    #print(X)

    X[j, 1:51] = torch.rand(50)

    #print(X)

    for i in range (51, 61):
      X[j, i] = X[j, 1] + 0.5* X[j, i - 50] + torch.randn(1)*np.sqrt(0.1)

    for i in range (61, 71):
      X[j, i] = X[j, i - 60] - X[j, i - 50] + X[j, i - 40] + ((torch.randn(1))/10)

    for i in range (71, 81):
      X[j, i] = X[j, 6*(i - 70)] + 3*X[j, i - 70] + ((torch.randn(1))/10)

    for i in range (81, 91):
      X[j, i] = 5 - X[j, i - 10]

    for i in range (91, 101):
      X[j, i] = 0.5*X[j, 50+(i - 90)*4] + 0.5*X[j, 50+(i - 90)*3] + ((torch.randn(1))/10)

    y[j] = sum(-0.88**i * X[j, 2*i] for i in range(1, 51)) + ((torch.randn(1))/100)

  return X, y

#Fitting a Linear Model
def compute_predictions(X, w):
  #Compute predictions: ypred = X * w
  ypred = torch.matmul(X, w)

  #if ypred.dim() == 2 and ypred.size(1) == 1:
        #ypred = ypred.view(-1)
  #print(ypred)
  return ypred

def compute_loss(y, ypred):
  # torch.mean((ypred - y) **2)
  #print("Squared differences:\n", squared_diffs)

  # calculate the average of the squared differences
  #average_squared_error = torch.mean(squared_diffs)

  mse = torch.nn.MSELoss()
  average_squared_error = mse(y, ypred)
  #print(average_squared_error)

  return average_squared_error

# Linear Regression
def split_data(X, y, train_ratio=0.7, test_ratio=0.2):
  #splits the data into train, test and validate!
  train_size = int(math.floor(train_ratio * len(X)))
  test_size = int(math.floor(test_ratio * len(X)))
  validate_size = len(X) - train_size - test_size
  X_train = torch.zeros(train_size, 101)
  X_test = torch.zeros(test_size, 101)
  X_validate = torch.zeros(validate_size, 101)
  y_train = torch.zeros(train_size, 1)
  y_test = torch.zeros(test_size, 1)
  y_validate = torch.zeros(validate_size, 1)

  X_train = X[:train_size].clone()
  y_train = y[:train_size].clone()
  X_test = X[train_size:train_size + test_size].clone()
  y_test = y[train_size:train_size + test_size].clone()
  X_validate = X[train_size + test_size:].clone()
  y_validate = y[train_size + test_size:].clone()

  return X_train, y_train, X_test, y_test, X_validate, y_validate

N=100
train_losses = []
test_losses = []

(X, y) = create_data(N)
w = torch.randn(101, 1, requires_grad=True)
(X_train, y_train, X_test, y_test, X_validate, y_validate) = split_data(X, y)
alpha = 0.0001
#end condition: reaching a fixed number of iterations
iterations = 100

#print(list(torch.utils.data.DataLoader(X_train)))

for i in range(iterations):
    # Training
    ypred_train = compute_predictions(X_train, w)
    # gradient descent
    loss_train = compute_loss(y_train, ypred_train)
    train_losses.append(loss_train.item())
    #print(loss_train)
    loss_train.backward()

    # Update weights
    with torch.no_grad():
        w -= alpha * w.grad

    w.grad.zero_()

    # Testing
    with torch.no_grad():
        ypred_test = compute_predictions(X_test, w)
        loss_test = compute_loss(y_test, ypred_test)
        test_losses.append(loss_test.item())

#print(train_losses, test_losses)
plt.plot(train_losses, label='Training Loss')
plt.plot(test_losses, label='Testing Loss')
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.legend()
plt.show()

def  train_model(X, y, w, alpha, iterations=1000):
  for i in range(iterations):
    # Training
    ypred = compute_predictions(X, w)
    # gradient descent
    loss_train = compute_loss(y, ypred)
    train_losses.append(loss_train.item())
    #print(loss_train)
    loss_train.backward()

    # Update weights
    with torch.no_grad():
        w -= alpha * w.grad
        w.grad.zero_()

N=1000
train_losses = []
test_losses = []

(X, y) = create_data(N)
(X_train, y_train, X_test, y_test, X_validate, y_validate) = split_data(X, y)
alpha = .001
step = .0001
w = torch.randn(101, 1, requires_grad=True)
for j in range(100):
  w = torch.randn(101, 1, requires_grad=True)
  train_model(X_train, y_train, w, alpha, 1000)
  ypred = compute_predictions(X_test, w)
  err = compute_loss(y_test, ypred).item()
  print( err, alpha)
  alpha += step

def  train_model(X, y, w, alpha, iterations=100):
  for i in range(iterations):
    # Training
    ypred = compute_predictions(X, w)
    # gradient descent
    loss_train = compute_loss(y, ypred)
    train_losses.append(loss_train.item())
    #print(loss_train)
    loss_train.backward()

    # Update weights
    with torch.no_grad():
        w -= alpha * w.grad
        w.grad.zero_()

N=1000
train_losses = []
test_losses = []
alpha = .001
(X_test, y_test) = create_data(200)
(X, y) = create_data(1000)
w = torch.randn(101, 1, requires_grad=True)
train_model(X_train, y_train, w, alpha, 1000)
ypred = compute_predictions(X_test, w)
err = compute_loss(y_test, ypred).item()
print(f'for 1000 {err}')


(X, y) = create_data(500)
w = torch.randn(101, 1, requires_grad=True)
train_model(X_train, y_train, w, alpha, 1000)
ypred = compute_predictions(X_test, w)
err = compute_loss(y_test, ypred).item()
print(f'for 500 {err}')


(X, y) = create_data(250)
w = torch.randn(101, 1, requires_grad=True)
train_model(X_train, y_train, w, alpha, 1000)
ypred = compute_predictions(X_test, w)
err = compute_loss(y_test, ypred).item()
print(f'for 250 {err}')


(X, y) = create_data(100)
w = torch.randn(101, 1, requires_grad=True)
train_model(X_train, y_train, w, alpha, 1000)
ypred = compute_predictions(X_test, w)
err = compute_loss(y_test, ypred).item()
print(f'for 100 {err}')

(X, y) = create_data(50)
w = torch.randn(101, 1, requires_grad=True)
train_model(X_train, y_train, w, alpha, 1000)
ypred = compute_predictions(X_test, w)
err = compute_loss(y_test, ypred).item()
print(f'for 50 {err}')

#Ridge Loss
mse = torch.nn.MSELoss()
def ridge_train(X, y, w, alpha, lamb):
  epoch = 200
  for j in range(200):
    ypred = compute_predictions(X, w)
    l2_norm = torch.sum(torch.square(w))
    loss = mse(ypred, y) + lamb*l2_norm
    loss.backward()

    with torch.no_grad():
      w += alpha * w.grad
    w.grad.zero_()
  return w

losses = []
lambs = []
N = 1000
alpha = .00001
(X, y) = create_data(300)
(X_test, y_test) = create_data(200)
lamb = 1
for i in np.arange(0.01, 5, 0.125):
  lambs.append(i)
  w = torch.rand(101, 1, requires_grad=True)
  w = ridge_train(X, y, w, alpha, lamb)
  ypred = compute_predictions(X_test, w)
  loss = mse(ypred, y_test)
  losses.append(loss.item())

plt.plot(lambs, losses, marker='o')
plt.xlabel('lambda')
plt.ylabel('Test Loss')
plt.title('Test Loss as a Function of Lambda')
plt.show()

#Lasso Regression
mse = torch.nn.MSELoss()
def lasso_train(X, y, w, alpha, beta):
  epoch = 200
  for j in range(epoch):
    ypred = compute_predictions(X, w)
    l1_norm = torch.sum(torch.abs(w))
    loss = mse(ypred, y) + l1_norm * beta
    loss.backward()
    with torch.no_grad():
      w += alpha * w.grad
    w.grad.zero_()
  return w

betas = np.zeros(101)
beta_zero = []
Nzeros = []
N=1000
alpha = .001
(X, y) = create_data(300)
(X_test, y_test) = create_data(200)
beta = 0.001
weights = []
weights = np.zeros(101)
for i in range(0, 101):
  w = torch.randn(101, 1, requires_grad=True)
  w = lasso_train(X, y, w, alpha, beta)
  ypred = compute_predictions(X_test, w)
  loss = mse(ypred, y_test)
  beta += 0.001
  nonzero = 0
  for s in range(0, 101):
    if w[s] > 0 and betas[s] == 0:
      betas[s] = beta
  for s in range(0, 101):
    weights[s] = w[s]

  Nzero = 0
  for s in range(0, 101):
    if w[s] > 0:
      Nzero += 1
  Nzeros.append(Nzero)
  beta_zero.append(beta)

# Plot
plt.plot(beta_zero, Nzeros, marker='o')
plt.xscale('log')  # Since betas vary in orders of magnitude
plt.xlabel('Beta (Regularization Strength)')
plt.ylabel('Number of Non-Zero Weights')
plt.title('Effect of Lasso Regularization on Weights Sparsity')
plt.show()

# Plot
plt.bar(beta_zero, betas, color ='maroon', width=0.0001)

plt.xlabel("Beta")
plt.ylabel("Beta where weight is 0")
plt.title("Killing th Feature")
plt.show()

#Under-Parameterization and Over-Parameterization
#fewer than 50 makes the data hard to classify

def m_matrix_train(X, y, w, alpha):
  for s in range(100):
    ypred = torch.matmul(X, w)
    mse = torch.nn.MSELoss()
    loss_5 = mse(y, ypred)
    print(loss_5)
    loss_5.backward()
    with torch.no_grad():
      w -= alpha * w.grad
    w.grad.zero_()
  return w

N=1000
alpha = .001
X = torch.zeros(50,50)
y = torch.zeros(50,1)
X_test = torch.zeros(50,50)
y_test = torch.zeros(50,1)
for i in range(50):
  X[i,0:50] = torch.rand(50)
  y[i] = torch.rand(1)
for i in range(50):
  X_test[i,0:50] = torch.rand(50)
  y_test[i] = torch.rand(1)

testing_error = []
given_k = []
for i in range(10, 50):
  w = torch.randn(i, 1, requires_grad=True)
  X_tr = torch.zeros(50,i)
  y_tr = torch.zeros(50,1)
  X_t1 = torch.zeros(50,i)
  y_t1 = torch.zeros(50,1)
  for j in range(0,50):
    for k in range(0,i):
      X_tr[j,k] = X[j,k]
      y_tr[j] = y[j]
      X_t1[j,k] = X_test[j,k]
      y_t1[j] = y_test[j]
  m_matrix_train(X_tr, y_tr, w, alpha)
  ypred = compute_predictions(X_t1, w)
  loss = mse(ypred, y_t1)
  given_k.append(i)
  testing_error.append(loss.item())

plt.plot(given_k, testing_error, marker='o', linestyle='-')
plt.xlabel('k (Number of Features)')
plt.ylabel('Testing Error')
plt.title('Testing Error as a Function of k')
plt.show()

#fewer than 50 makes the data hard to classify

mse = torch.nn.MSELoss()
def m_matrix_train(X, y, w, alpha):
   for i in range(1000):
     ypred = torch.matmul(X, w)
     loss = mse(ypred, y)
     loss.backward()
     with torch.no_grad():
       w -= alpha * w.grad
     w.grad.zero_()
   return w

N=1000
alpha = .001
X = torch.zeros(50,50)
y = torch.zeros(50,1)
X_test = torch.zeros(50,50)
y_test = torch.zeros(50,1)
for i in range(50):
  X[i,0:50] = torch.rand(50)
  y[i] = torch.rand(1)
for i in range(50):
  X_test[i,0:50] = torch.rand(50)
  y_test[i] = torch.rand(1)

for i in range(1,50):
  w = torch.randn(i, 1, requires_grad=True)
  X_tr = torch.zeros(50,i)
  y_tr = torch.zeros(50,1)
  X_t1 = torch.zeros(50,i)
  y_t1 = torch.zeros(50,1)
  for j in range(0,50):
    for k in range(0,i):
      X_tr[j,k] = X[j,k]
      y_tr[j] = y[j]
      X_t1[j,k] = X_test[j,k]
      y_t1[j] = y_test[j]
  w = m_matrix_train(X_tr, y_tr, w, alpha)
  ypred = compute_predictions(X_t1, w)
  loss = mse(ypred, y_t1)

plt.plot(range(1, 51), test_errors, marker='o', linestyle='-')
plt.xlabel('k (Number of Features)')
plt.ylabel('Testing Error')
plt.title('Testing Error as a Function of k')
plt.show()
